{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#M√©todos de Inicializa√ß√£o de Pesos"
      ],
      "metadata": {
        "id": "90Mkn1cw5IK5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os m√©todos de inicializa√ß√£o de pesos em redes neurais s√£o t√©cnicas utilizadas para definir os valores iniciais dos par√¢metros de uma rede antes do treinamento.\n",
        "\n",
        "A escolha da inicializa√ß√£o pode impactar diretamente a converg√™ncia do modelo, afetando a estabilidade do gradiente e a velocidade de aprendizado. Inicializa√ß√µes inadequadas podem levar a problemas como gradientes desaparecendo (vanishing gradients) ou gradientes explosivos (exploding gradients), dificultando o treinamento da rede.\n",
        "\n",
        "Para mitigar esses problemas, diferentes estrat√©gias foram desenvolvidas, como inicializa√ß√£o aleat√≥ria, Xavier/Glorot, He e at√© inicializa√ß√£o zero (que, apesar de ser um caso extremo, exemplifica a import√¢ncia de um bom m√©todo).\n",
        "\n",
        "O objetivo principal dessas abordagens √© garantir que os pesos sejam pequenos o suficiente para evitar explos√µes de gradientes, mas suficientemente diversos para que diferentes neur√¥nios aprendam representa√ß√µes distintas dos dados.\n",
        "\n"
      ],
      "metadata": {
        "id": "eORGGF0UhLFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Inicializa√ß√£o Aleat√≥ria"
      ],
      "metadata": {
        "id": "oE8hVJ0b5OmR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A inicializa√ß√£o aleat√≥ria de pesos √© uma das estrat√©gias mais simples utilizadas para definir os valores iniciais dos par√¢metros em redes neurais antes do treinamento. Seu principal objetivo √© evitar a simetria entre os neur√¥nios, garantindo que cada um aprenda diferentes padr√µes dos dados. Se todos os pesos fossem inicializados com zero, os neur√¥nios dentro de uma mesma camada receberiam gradientes id√™nticos, tornando-se redundantes e limitando a capacidade da rede. Para evitar esse problema, os pesos podem ser gerados aleatoriamente a partir de distribui√ß√µes uniformes ou normais (gaussianas). Na distribui√ß√£o uniforme, os pesos s√£o amostrados dentro de um intervalo\n",
        "[\n",
        "‚àí\n",
        "ùëü\n",
        ",\n",
        "ùëü\n",
        "]\n",
        ", enquanto na distribui√ß√£o normal, os pesos seguem uma distribui√ß√£o gaussiana com m√©dia zero e um desvio padr√£o controlado. No entanto, a inicializa√ß√£o aleat√≥ria pura pode causar problemas como gradientes explosivos (se os valores dos pesos forem muito grandes) ou gradientes desaparecendo (se forem muito pequenos), dificultando o treinamento da rede. Al√©m disso, a escolha inadequada da distribui√ß√£o pode interagir negativamente com fun√ß√µes de ativa√ß√£o como sigmoid e tanh, levando a satura√ß√£o e reduzindo a efici√™ncia da retropropaga√ß√£o. Por conta dessas limita√ß√µes, m√©todos mais avan√ßados, como Xavier/Glorot e He, foram desenvolvidos para ajustar dinamicamente os valores iniciais dos pesos de acordo com a arquitetura da rede, otimizando a propaga√ß√£o dos gradientes e melhorando a estabilidade do treinamento."
      ],
      "metadata": {
        "id": "Hl31iJHzUpze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Formulas\n",
        "\n"
      ],
      "metadata": {
        "id": "Ji5F0rB1UxjN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A inicializa√ß√£o aleat√≥ria de pesos em redes neurais √© essencial para evitar a simetria entre os neur√¥nios e permitir um aprendizado eficaz. Para isso, dois m√©todos comuns de gera√ß√£o de pesos s√£o a distribui√ß√£o uniforme e a distribui√ß√£o normal (gaussiana). Ambas s√£o utilizadas para definir valores iniciais dos pesos antes do treinamento da rede, mas cada uma tem suas caracter√≠sticas espec√≠ficas e aplica√ß√µes ideais."
      ],
      "metadata": {
        "id": "pf0iQBffeAWz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A inicializa√ß√£o uniforme define os pesos\n",
        "ùëä\n",
        "W dentro de um intervalo\n",
        "[\n",
        "‚àí\n",
        "ùëü\n",
        ",\n",
        "ùëü\n",
        "]\n",
        ", onde todos os valores dentro desse intervalo t√™m a mesma probabilidade de serem escolhidos. A f√≥rmula geral para a distribui√ß√£o uniforme √©:\n",
        "\n",
        "ùëä\n",
        "‚àº\n",
        "ùëà\n",
        "(\n",
        "‚àí\n",
        "ùëü\n",
        ",\n",
        "ùëü\n",
        ")\n",
        "\n",
        "Isso significa que os pesos s√£o amostrados de uma distribui√ß√£o uniforme no intervalo definido. A fun√ß√£o densidade de probabilidade (PDF) dessa distribui√ß√£o √© dada por:\n",
        "\n",
        "$$\\\n",
        "P(W) =\n",
        "\\begin{cases}\n",
        "\\frac{1}{2r}, & \\text{se } -r \\leq W \\leq r \\\\\n",
        "0, & \\text{caso contr√°rio}\n",
        "\\end{cases}$$\n",
        "\n",
        "Ou seja, todos os valores dentro do intervalo possuem a mesma probabilidade\n",
        "$ \\frac{1}{2r} $ , garantindo que os pesos sejam distribu√≠dos de maneira uniforme.\n",
        "\n",
        "O valor de\n",
        "ùëü pode ser determinado empiricamente ou definido com base no n√∫mero de neur√¥nios na camada. Se os pesos forem muito grandes, podem causar exploding gradients; se forem muito pequenos, podem levar a vanishing gradients.\n",
        "\n",
        "Essa inicializa√ß√£o tem algumas vantagens e desvantagens, como as seguintes:\n",
        "\n",
        "Vantagem: Simplicidade e controle expl√≠cito sobre o intervalo dos pesos.\n",
        "\n",
        "Desvantagem: Pode n√£o ser ideal para fun√ß√µes de ativa√ß√£o como sigmoid e tanh, pois n√£o leva em conta a varia√ß√£o do gradiente.\n",
        "\n"
      ],
      "metadata": {
        "id": "W0yTVK8dWwoQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outra abordagem comum √© inicializar os pesos seguindo uma distribui√ß√£o normal (gaussiana) com m√©dia zero e vari√¢ncia\n",
        "ùúé\n",
        "2\n",
        " , ou seja:\n",
        "\n",
        "ùëä\n",
        "‚àº\n",
        "ùëÅ\n",
        "(\n",
        "0\n",
        ",\n",
        "ùúé\n",
        "2\n",
        ")\n",
        "\n",
        "A fun√ß√£o densidade de probabilidade (PDF) para a distribui√ß√£o normal √©:\n",
        "\n",
        "$$\n",
        "P(W) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{W^2}{2\\sigma^2}}\n",
        "$$\n",
        "\n",
        "Isso significa que os pesos estar√£o concentrados em torno da m√©dia 0, com maior probabilidade de assumir valores pr√≥ximos de zero e menos probabilidade de assumir valores extremos.\n",
        "\n",
        "Escolha de\n",
        "ùúé\n",
        ": O desvio padr√£o\n",
        "ùúé\n",
        " precisa ser ajustado para garantir que os gradientes n√£o sejam muito grandes nem muito pequenos.\n",
        "Existem algumas vantagens desse m√©todo de inicia√ß√£o, e desventagens, descritas a seguir:\n",
        "\n",
        "Vantagem: Apresenta Vari√¢ncia do Gradiente equilibrada entre as camadas, Melhor compatibilidade com redes profundas e algumas fun√ß√µes de ativa√ß√£o, al√©m de  resolver o problema de Simetria entre Neur√¥nios ao atribuir pesos ligeiramente diferentes para cada neur√¥nio, permitindo que cada um aprenda representa√ß√µes distintas.\n",
        "\n",
        "Desvantagem: Pode ser sens√≠vel ao valor escolhido para\n",
        "ùúé\n",
        "œÉ, exigindo ajuste cuidadoso."
      ],
      "metadata": {
        "id": "7ZpgEjbkvfES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Inicializa√ß√£o Xavier/Glorot"
      ],
      "metadata": {
        "id": "XkYdsyms8Ixe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Inicializa√ß√£o He"
      ],
      "metadata": {
        "id": "VBfegN988Nxd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Inicializa√ß√£o Zero"
      ],
      "metadata": {
        "id": "tWkdEcFb9lZR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Compara√ß√£o Com Experimento"
      ],
      "metadata": {
        "id": "6v1qt2569oFm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Impacto da Inicializa√ß√£o de Pesos\n",
        "\n"
      ],
      "metadata": {
        "id": "nzq6Sw9h-juD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import√¢ncia da inicializa√ß√£o de pesos\n"
      ],
      "metadata": {
        "id": "p07tpPZW-xk8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Problemas causados por inicializa√ß√£o inadequada:\n"
      ],
      "metadata": {
        "id": "ubbp022l-0E3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Gradientes explosivos (exploding gradients)\n"
      ],
      "metadata": {
        "id": "cM2HbeZD-4br"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Gradientes desaparecendo (vanishing gradients)"
      ],
      "metadata": {
        "id": "aoeyrRUC_FbH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testes pr√°ticos comparando os m√©todos de inicializa√ß√£o"
      ],
      "metadata": {
        "id": "EyM7fv5u_MLS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gr√°ficos de converg√™ncia e impacto na performance"
      ],
      "metadata": {
        "id": "vEcFmr1C-9Pj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0-6AjB44kwg"
      },
      "outputs": [],
      "source": []
    }
  ]
}